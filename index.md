---
title: Spark Performance Analysis
layout: default
---
# Spark Performance Analysis

## Introduction

There has been significant effort dedicated towards improving the performance of data analytics frameworks, but comparatively little effort has been spent systematically identifying the performance bottlenecks of these systems.  We set out to make data analytics framework performance easier to understand, both by quantifying the performance bottlenecks in distributed computation frameworks on a few benchmarks, and by providing tools that others can use to understand performance of the workloads we ran and of their own workloads.

This project is summarized in a [paper](http://www.eecs.berkeley.edu/~keo/publications/nsdi15-final147.pdf) that will appear at USENIX NSDI, 2015.

## Traces

We collected JSON logs from running the big data benchmark and TPC-DS benchmark on clusters of 5-60 
machines. All of the traces are licensed under a
[Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).

### How the traces were generated

The JSON logs linked below were generated by the Spark driver (version 1.2.1) while running various benchmarks
on clusters of Amazon EC2 machines.  All of the below traces were generated from clusters of
m2.4xlarge machines, which each have 68.4GiB of memory, 2 840GB disks, 8 cores, and a 1Gbps network
link.

These logs were enabled by
setting `spark.eventLog.enabled` to `true` in the Spark configuration when running benchmark
queries (see the
[Spark documentation](http://spark.apache.org/docs/1.2.0/configuration.html) for details).

### Trace format

The traces are formatted as JSON data; if you're interested in the nitty gritty,
[this file](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala)
in the Spark code base generates the data.  The trace includes a JSON object for each time
a job starts and ends, each time a stage starts and ends, and each time a task starts and ends.
The most useful information is the per-task information, which logs detailed information about the
execution of each task (e.g., how long the task spent blocked on the network, blocked on disk
I/O, performing Java garbage collection, and more).

### Available traces

The traces labeled as TPC-DS ran a modified version of the [TPC-DS benchmark](http://www.tpc.org/tpcds/).
The modified version runs a subset of 20 queries selected by an
[industry benchmark](http://blog.cloudera.com/blog/2014/01/impala-performance-dbms-class-speed/).

<table class="table table-striped">
  <thead>
    <tr>
      <td><b>Link</b></td>
      <td><b>Benchmark</b></td>
      <td><b>Number of machines</b></td>
      <td><b>Data Format</b></td>
      <td><b>Data Size</b></td>
      <td><b>Concurrent Users</b></td>
      <td><b>Notes</b></td>
    </tr>
  </thead>
  <tbody>
    <tr> 
      <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_01_bdb_sf5_48g_disk_event_log">Trace</a></td>
      <td> <a href="https://amplab.cs.berkeley.edu/benchmark/">Big data benchmark</a> </td>
      <td> 5 </td>
      <td> Compressed, Disk </td>
      <td> 60GB </td>
      <td> 1 </td>
      <td> This trace includes 6 trials; in each trial, the 10 queries were executed in random order. </td>
    </tr>
    <tr>
      <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_01_bdb_sf5_48g_memory_event_log">Trace</a></td>
      <td> <a href="https://amplab.cs.berkeley.edu/benchmark/">Big data benchmark</a> </td>
      <td> 5 </td>
      <td> SparkSQL Columnar Cache </td>
      <td> 60GB </td>
      <td> 1 </td>
      <td> This trace includes 6 trials; in each trial, the 10 queries were executed in random order. </td>
    </tr>
    <tr>
      <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_01_tpcds_sf5000_disk_event_log">Trace</a></td>
      <td> TPC-DS </td>
      <td> 20 </td>
      <td> Parquet, Disk </td>
      <td> 850GB </td>
      <td> 13 </td>
      <td> This trace includes a warmup period where each of the queries is run once. After the
        warmup period completes, the 13 users run in parallel. Each user runs the 20 queries in
        series, in a random order.
      </td>
    </tr>
    <tr>
      <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_03_tpcds_sf5000_1user_6iterations_event_log">Trace</a></td>
      <td> TPC-DS </td>
      <td> 20 </td>
      <td> Parquet, Disk </td>
      <td> 850GB </td>
      <td> 1 </td>
      <td> The trace includes 6 trials, where in each trial, the 20 queries are executed in
      series. This trace is useful for understanding the total CPU, network, and disk resources
      consumed by each query (understanding the resource consumption of each query cannot
      be done from the experiment with many multiple users, because the CPU, network, and disk
      counters reflect all tasks running on each machine, so when one machine is executing
      multiple queries, there's no way to determine which resources were used by which
      query.
      </td>
    </tr>
    <tr>
      <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_02_tpcds_sf100_20machines_memory">Trace</a></td>
      <td> TPC-DS </td>
      <td> 20 </td>
      <td> Spark SQL Columnar Cache </td>
      <td> 17GB when stored as on-disk Parquet files; 200GB in-memory </td>
      <td> 7 </td>
      <td> This trace includes a warmup period where each of the queries is run once. After the
        warmup period completes, the 7 users run in parallel. Each user runs the 20 queries in
        series, in a random order.
      </td>
    </tr>
    <tr>
      <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_02_tpcds_1user_sf100_memory_6iterations_event_log">Trace</a></td>
      <td> TPC-DS </td>
      <td> 20 </td>
      <td> Spark SQL Columnar Cache </td>
      <td> 17GB when stored as on-disk Parquet files; 200GB in-memory </td>
      <td> 1 </td>
      <td> The trace includes 6 trials, where in each trial, the 20 queries are executed in
      series. This trace is useful for understanding the total CPU, network, and disk resources
      consumed by each query (understanding the resource consumption of each query cannot
      be done from the experiment with many multiple users, because the CPU, network, and disk
      counters reflect all tasks running on each machine, so when one machine is executing
      multiple queries, there's no way to determine which resources were used by which
      query.
      </td>
    </tr>
    <tr>
     <td> <a href="http://eecs.berkeley.edu/~keo/traces/1.2.1/2015_03_03_tpcds_sf15000_60machines_15users_disk_event_log">Trace</a></td>
     <td> TPC-DS </td>
     <td> 60 </td>
     <td> Parquet, Disk </td>
     <td> 2500GB </td>
     <td> 15 </td>
     <td> This trace includes a warmup period where each of the queries is run once. After the
       warmup period completes, the 15 users run in parallel. Each user runs the 20 queries in
       series, in a random order.
     </td>
   </tr>
  </tbody>
</table>

## Performance Visualization

A suite of visualization tools that you can use to understand Spark's performance are publicly
available [here](https://github.com/kayousterhout/trace-analysis).  The README on that page
describes how those scripts can be used. Those scripts can be used to generate 
easy-to-understand performance visualizations, as well as to generate all of the
data that was included in our paper.

We are also in the process of adding the performance visualization created by that tool
to Spark's UI, a feature that we expect to be part of the Spark 1.4.0 release. For more
information and to track progress of this feature, see the
[JIRA issue](https://issues.apache.org/jira/browse/SPARK-6418).

## Contact

If you have questions about this project, the performance analysis tools, or the traces,
contact [Kay Ousterhout](http://eecs.berkeley.edu/~keo).

